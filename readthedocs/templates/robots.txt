# This robots.txt file is autogenerated by Read the Docs.
# It controls the crawling and indexing of your documentation by search engines.
#
# You can learn more about robots.txt, including how to customize it, in our documentation:
#
# * Our documentation on Robots.txt: https://docs.readthedocs.com/platform/stable/reference/robots.html
# * Our guide about SEO techniques: https://docs.readthedocs.com/platform/stable/guides/technical-docs-seo-guide.html

User-agent: *
{% for path in hidden_paths %}
Disallow: {{ path }} # Hidden version
{% empty %}
Disallow: # Allow everything
{% endfor %}
Sitemap: {{ sitemap_url }}
